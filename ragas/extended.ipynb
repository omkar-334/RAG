{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'sudo' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install poppler-utils tesseract-ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.chdir(r'C:\\Users\\DELL\\bhabha ai\\Ragas_repo\\ragas\\src')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0R3ubrNuXc_0"
      },
      "outputs": [],
      "source": [
        "# Run 'python -m vllm.entrypoints.openai.api_server --model explodinggradients/Ragas-critic-llm-Qwen1.5-GPTQ' on cmd with gpu \n",
        "# inference_server_url = \"http://localhost:8000/v1\"\n",
        "# from langchain_openai import ChatOpenAI\n",
        "# MODEL = \"explodinggradients/Ragas-critic-llm-Qwen1.5-GPTQ\"\n",
        "# chat = ChatOpenAI(\n",
        "#     model=MODEL,\n",
        "#     openai_api_key=\"token-abc123\",\n",
        "#     openai_api_base=inference_server_url,\n",
        "#     max_tokens=2048,\n",
        "#     temperature=0,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "MODEL = \"explodinggradients/Ragas-critic-llm-Qwen1.5-GPTQ\"\n",
        "chat = ChatOpenAI(\n",
        "    model=MODEL,\n",
        "    openai_api_key=\"\",\n",
        "    openai_api_base=\"https://api.runpod.ai/v2/1lh33m6cyf1cue/openai/v1\",\n",
        "    max_tokens=2048,\n",
        "    temperature=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "B-fKJ3_sY96K"
      },
      "outputs": [],
      "source": [
        "distributions = {simple: 0.3, reasoning: 0.3, multi_context: 0.4}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\omkar\\\\Desktop\\\\Ragasexp'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "loader = PyPDFLoader(r\"pdffiles/file1.pdf\")\n",
        "documents = loader.load()\n",
        "pages = loader.load_and_split(CharacterTextSplitter(separator='.\\n'))\n",
        "# pages = loader.load_and_split(RecursiveCharacterTextSplitter(separator='.\\n'))\n",
        "text_splitter = CharacterTextSplitter(separator='.\\n')\n",
        "docs = text_splitter.split_documents(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# loader = DirectoryLoader(\"./AI Domain/\", glob=\"*.pdf\")\n",
        "# documents = loader.load()\n",
        "\n",
        "# for document in documents:\n",
        "#     document.metadata[\"filename\"] = document.metadata[\"source\"]\n",
        "\n",
        "# documents = [doc for doc in documents if len(doc.page_content.split()) > 5000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UHXfZKIHYjZt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\omkar\\Desktop\\Ragasexp\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Filename and doc_id are the same for all nodes.                 \n",
            "Generating:   0%|          | 0/15 [00:00<?, ?it/s]Failed to parse output. Returning None.\n",
            "Generating:   0%|          | 0/15 [00:28<?, ?it/s]\n",
            "Exception in thread Thread-21:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\omkar\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"c:\\Users\\omkar\\Desktop\\Ragasexp\\.venv\\lib\\site-packages\\ragas\\executor.py\", line 95, in run\n",
            "    results = self.loop.run_until_complete(self._aresults())\n",
            "  File \"C:\\Users\\omkar\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\asyncio\\base_events.py\", line 647, in run_until_complete\n",
            "    return future.result()\n",
            "  File \"c:\\Users\\omkar\\Desktop\\Ragasexp\\.venv\\lib\\site-packages\\ragas\\executor.py\", line 83, in _aresults\n",
            "    raise e\n",
            "  File \"c:\\Users\\omkar\\Desktop\\Ragasexp\\.venv\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"C:\\Users\\omkar\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\asyncio\\tasks.py\", line 611, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"c:\\Users\\omkar\\Desktop\\Ragasexp\\.venv\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"c:\\Users\\omkar\\Desktop\\Ragasexp\\.venv\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"c:\\Users\\omkar\\Desktop\\Ragasexp\\.venv\\lib\\site-packages\\ragas\\testset\\evolutions.py\", line 142, in evolve\n",
            "    ) = await self._aevolve(current_tries, current_nodes)\n",
            "  File \"c:\\Users\\omkar\\Desktop\\Ragasexp\\.venv\\lib\\site-packages\\ragas\\testset\\evolutions.py\", line 551, in _aevolve\n",
            "    result = await self._acomplex_evolution(\n",
            "  File \"c:\\Users\\omkar\\Desktop\\Ragasexp\\.venv\\lib\\site-packages\\ragas\\testset\\evolutions.py\", line 379, in _acomplex_evolution\n",
            "    simple_question, current_nodes, _ = await self.se._aevolve(\n",
            "  File \"c:\\Users\\omkar\\Desktop\\Ragasexp\\.venv\\lib\\site-packages\\ragas\\testset\\evolutions.py\", line 296, in _aevolve\n",
            "    passed = await self.node_filter.filter(merged_node)\n",
            "  File \"c:\\Users\\omkar\\Desktop\\Ragasexp\\.venv\\lib\\site-packages\\ragas\\testset\\filters.py\", line 60, in filter\n",
            "    output[\"score\"] = sum(output.values()) / len(output.values())\n",
            "ZeroDivisionError: division by zero\n"
          ]
        },
        {
          "ename": "ExceptionInRunner",
          "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 29\u001b[0m\n\u001b[0;32m     17\u001b[0m generator \u001b[38;5;241m=\u001b[39m TestsetGenerator\u001b[38;5;241m.\u001b[39mfrom_langchain(\n\u001b[0;32m     18\u001b[0m     generator_llm,\n\u001b[0;32m     19\u001b[0m     critic_llm,\n\u001b[0;32m     20\u001b[0m     embeddings\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m distributions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     24\u001b[0m     simple: \u001b[38;5;241m0.33\u001b[39m,\n\u001b[0;32m     25\u001b[0m     multi_context: \u001b[38;5;241m0.33\u001b[39m,\n\u001b[0;32m     26\u001b[0m     reasoning: \u001b[38;5;241m0.34\u001b[39m\n\u001b[0;32m     27\u001b[0m }\n\u001b[1;32m---> 29\u001b[0m testset \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_langchain_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistributions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# testset = generator.generate_with_langchain_docs(\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#     documents[:10],\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#     test_size=5,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#     distributions=distributions,\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\omkar\\Desktop\\Ragasexp\\.venv\\lib\\site-packages\\ragas\\testset\\generator.py:210\u001b[0m, in \u001b[0;36mTestsetGenerator.generate_with_langchain_docs\u001b[1;34m(self, documents, test_size, distributions, with_debugging_logs, is_async, raise_exceptions, run_config)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# chunk documents and add to docstore\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocstore\u001b[38;5;241m.\u001b[39madd_documents(\n\u001b[0;32m    207\u001b[0m     [Document\u001b[38;5;241m.\u001b[39mfrom_langchain_document(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    208\u001b[0m )\n\u001b[1;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_debugging_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_debugging_logs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_async\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraise_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\omkar\\Desktop\\Ragasexp\\.venv\\lib\\site-packages\\ragas\\testset\\generator.py:305\u001b[0m, in \u001b[0;36mTestsetGenerator.generate\u001b[1;34m(self, test_size, distributions, with_debugging_logs, is_async, raise_exceptions, run_config)\u001b[0m\n\u001b[0;32m    303\u001b[0m     test_data_rows \u001b[38;5;241m=\u001b[39m exec\u001b[38;5;241m.\u001b[39mresults()\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m test_data_rows:\n\u001b[1;32m--> 305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "\u001b[1;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
          ]
        }
      ],
      "source": [
        "from ragas.testset.generator import TestsetGenerator\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
        "\n",
        "# # generator with custom llm and embeddings\n",
        "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\",openai_api_key='')\n",
        "critic_llm = ChatOpenAI(\n",
        "    model=\"explodinggradients/Ragas-critic-llm-Qwen1.5-GPTQ\",\n",
        "    openai_api_key=\"\",\n",
        "    openai_api_base=\"https://api.runpod.ai/v2/1lh33m6cyf1cue/openai/v1\",\n",
        "    max_tokens=2048,\n",
        "    temperature=0,\n",
        ")\n",
        "embeddings = OpenAIEmbeddings(openai_api_key='')\n",
        "# embeddings = TogetherEmbeddings(model=\"togethercomputer/m2-bert-80M-8k-retrieval\")\n",
        "\n",
        "generator = TestsetGenerator.from_langchain(\n",
        "    generator_llm,\n",
        "    critic_llm,\n",
        "    embeddings\n",
        ")\n",
        "\n",
        "distributions = {\n",
        "    simple: 0.33,\n",
        "    multi_context: 0.33,\n",
        "    reasoning: 0.34\n",
        "}\n",
        "\n",
        "testset = generator.generate_with_langchain_docs(docs, 15, distributions)\n",
        "# testset = generator.generate_with_langchain_docs(\n",
        "#     documents[:10],\n",
        "#     test_size=5,\n",
        "#     raise_exceptions=False,\n",
        "#     with_debugging_logs=False,\n",
        "#     distributions=distributions,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "Dataframe = testset.to_pandas()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
